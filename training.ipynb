{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_absolute_percentage_error, mean_squared_error\n",
    "from sklearn.model_selection import KFold, RandomizedSearchCV, cross_val_score, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from tensorflow.keras import layers, models, optimizers, losses\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import statistics\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55284, 30)\n",
      "(29769, 30)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_pickle('preprocess_data.pkl')\n",
    "# data = pd.read_csv('preprocess_data.csv', encoding='utf-8', encoding_errors='ignore')\n",
    "train = data.loc[data.flag=='train',:]\n",
    "print(train.shape)\n",
    "test = data.loc[data.flag=='test',:]\n",
    "print(test.shape)\n",
    "\n",
    "train.drop(columns=['flag'],inplace=True)\n",
    "test.drop(columns=['flag'],inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neighborhood_overview          19506\n",
       "host_response_time             17802\n",
       "host_response_rate             17802\n",
       "space                          16881\n",
       "summary                         2954\n",
       "host_has_profile_pic             111\n",
       "host_identity_verified           111\n",
       "host_since                       111\n",
       "host_is_superhost                111\n",
       "name                              14\n",
       "guests_included                    0\n",
       "review_scores_value                0\n",
       "review_scores_location             0\n",
       "review_scores_communication        0\n",
       "reviews_per_month                  0\n",
       "review_scores_checkin              0\n",
       "review_scores_cleanliness          0\n",
       "review_scores_accuracy             0\n",
       "review_scores_rating               0\n",
       "bedrooms                           0\n",
       "price                              0\n",
       "amenities                          0\n",
       "bed_type                           0\n",
       "beds                               0\n",
       "bathrooms                          0\n",
       "accommodates                       0\n",
       "property_type                      0\n",
       "host_total_listings_count          0\n",
       "listing_id                         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name                           14\n",
       "beds                            0\n",
       "reviews_per_month               0\n",
       "review_scores_value             0\n",
       "review_scores_location          0\n",
       "review_scores_communication     0\n",
       "review_scores_checkin           0\n",
       "review_scores_cleanliness       0\n",
       "review_scores_accuracy          0\n",
       "review_scores_rating            0\n",
       "guests_included                 0\n",
       "price                           0\n",
       "amenities                       0\n",
       "bed_type                        0\n",
       "bedrooms                        0\n",
       "summary                         0\n",
       "bathrooms                       0\n",
       "accommodates                    0\n",
       "property_type                   0\n",
       "host_identity_verified          0\n",
       "host_has_profile_pic            0\n",
       "host_total_listings_count       0\n",
       "host_is_superhost               0\n",
       "host_response_rate              0\n",
       "host_response_time              0\n",
       "host_since                      0\n",
       "neighborhood_overview           0\n",
       "space                           0\n",
       "listing_id                      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### Null treatment\n",
    "\n",
    "def treat_null(df):\n",
    "    for c in ['host_has_profile_pic','host_is_superhost','host_identity_verified','host_response_rate','host_response_time']:\n",
    "        df[c].fillna(0,inplace=True)\n",
    "    df['neighborhood_overview'].fillna('',inplace=True)\n",
    "    df['space'].fillna('',inplace=True)\n",
    "    df['summary'].fillna('',inplace=True)\n",
    "    df['host_since'].fillna(statistics.median(df['host_since'].dropna().tolist()),inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "train = treat_null(train)\n",
    "test = treat_null(test)\n",
    "\n",
    "train.isnull().sum().sort_values(ascending=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Feature identification\n",
    "\n",
    "target_col = 'price'\n",
    "id_col = 'listing_id'\n",
    "text_cols = ['name','summary','space','neighborhood_overview']\n",
    "numeric_cols = [c for c in train.columns if c not in [target_col]+[id_col]+text_cols]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling with only numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = train.dropna()\n",
    "X = features[[c for c in numeric_cols]]\n",
    "y = features[target_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55270, 23)\n",
      "55270\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val,y_train,y_val = train_test_split(X,y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: mean 0.55 std 0.008\n"
     ]
    }
   ],
   "source": [
    "# feature selection\n",
    "\n",
    "def select_linear_features(X_train, y_train, X_val, score_func):\n",
    "\tscale = StandardScaler().fit(X_train)\n",
    "\tX_train = scale.transform(X_train)\n",
    "\tX_val = scale.transform(X_train)\n",
    "\tfs = SelectKBest(score_func=score_func, k=15)\n",
    "\tfs.fit(X_train, y_train)\n",
    "\tX_train_fs = fs.transform(X_train)\n",
    "\tX_val_fs = fs.transform(X_val)\n",
    "\treturn X_train_fs, X_val_fs\n",
    "\n",
    "\n",
    "\n",
    "cv = KFold(n_splits=10)\n",
    "# feature selection\n",
    "X_train_k, X_val_k = select_linear_features(X_train, y_train, X_val, mutual_info_regression)\n",
    "model = LinearRegression()\n",
    "cv_score = cross_val_score(model,X_train_k,y_train,scoring='neg_mean_absolute_percentage_error',cv=cv)\n",
    "print(\"MSE: mean {:.2f} std {:.3f}\".format(statistics.mean(cv_score)*-1, statistics.stdev(cv_score)))\n",
    "\n",
    "# model.fit(X_train_fs, y_train)\n",
    "# # evaluate the model\n",
    "# yhat = model.predict(X_test_fs)\n",
    "# # evaluate predictions\n",
    "# mae = mean_absolute_error(y_test, yhat)\n",
    "# print('MAE: %.3f' % mae)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stochastic Gradient Descent Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RandomForest Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GradientBoosting Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling with Text data included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### load word2vec embeddings\n",
    "\n",
    "\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "phrases = Phrases.load('bigram_model.pkl')\n",
    "\n",
    "weights = KeyedVectors.load_word2vec_format('w2v_tmp.model')\n",
    "vocab_size = len(weights)\n",
    "vocab = [weights.index_to_key[i] for i in range(vocab_size)]\n",
    "embedding_matrix = weights.vectors\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(features['summary'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>summary</th>\n",
       "      <th>space</th>\n",
       "      <th>neighborhood_overview</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>attic room historic greenwich</td>\n",
       "      <td>room door discreet staircase light airy open p...</td>\n",
       "      <td>double room available historic royal greenwich...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lovely garden studio private access</td>\n",
       "      <td>garden studio private entrance minute crouch e...</td>\n",
       "      <td>beautiful studio king size bed sofa coffee tab...</td>\n",
       "      <td>crouch end hip friendly neighbourhood filled e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>comfi apartment close wimbledon tennis court</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>luxury room heart london sw</td>\n",
       "      <td>luxury first floor victorian split level maiso...</td>\n",
       "      <td>room fully furnished include double bed mirror...</td>\n",
       "      <td>gail coffee shop round corner well local indep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>excellent city apartment private patio</td>\n",
       "      <td>new luxury apartment private outside patio gre...</td>\n",
       "      <td>modern well equipped cosy apartment close vict...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           name  \\\n",
       "0                 attic room historic greenwich   \n",
       "1           lovely garden studio private access   \n",
       "2  comfi apartment close wimbledon tennis court   \n",
       "3                   luxury room heart london sw   \n",
       "4        excellent city apartment private patio   \n",
       "\n",
       "                                             summary  \\\n",
       "0  room door discreet staircase light airy open p...   \n",
       "1  garden studio private entrance minute crouch e...   \n",
       "2                                                      \n",
       "3  luxury first floor victorian split level maiso...   \n",
       "4  new luxury apartment private outside patio gre...   \n",
       "\n",
       "                                               space  \\\n",
       "0  double room available historic royal greenwich...   \n",
       "1  beautiful studio king size bed sofa coffee tab...   \n",
       "2                                                      \n",
       "3  room fully furnished include double bed mirror...   \n",
       "4  modern well equipped cosy apartment close vict...   \n",
       "\n",
       "                               neighborhood_overview  \n",
       "0                                                     \n",
       "1  crouch end hip friendly neighbourhood filled e...  \n",
       "2                                                     \n",
       "3  gail coffee shop round corner well local indep...  \n",
       "4                                                     "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_features = features[[c for c in text_cols]]\n",
    "for c in text_features.columns:\n",
    "    text_features[c] = text_features.apply(lambda x: ' '.join(x[c]), axis=1)\n",
    "\n",
    "text_features.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 231.     350.     473.     522.    1026.193]\n",
      "[   0.  205.  565.  953. 1157.]\n",
      "[42. 51. 59. 65. 79.]\n",
      "[   0.  163.  423.  826. 1097.]\n"
     ]
    }
   ],
   "source": [
    "sumamry = np.array(list(map(lambda x:len(x), features['summary'].tolist())))\n",
    "print(np.percentile(sumamry,[25,50,75,90,99.7],))\n",
    "\n",
    "\n",
    "space = np.array(list(map(lambda x:len(x), features['space'].tolist())))\n",
    "print(np.percentile(space,[25,50,75,90,99.7],))\n",
    "\n",
    "\n",
    "name = np.array(list(map(lambda x:len(x), features['name'].tolist())))\n",
    "print(np.percentile(name,[25,50,75,90,99.7],))\n",
    "\n",
    "\n",
    "neighborhood_overview = np.array(list(map(lambda x:len(x), features['neighborhood_overview'].tolist())))\n",
    "print(np.percentile(neighborhood_overview,[25,50,75,90,99.7],))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        b'room door discreet staircase light airy open...\n",
       "1        b'garden studio private entrance minute crouch...\n",
       "2                                                      b''\n",
       "3        b'luxury first floor victorian split level mai...\n",
       "4        b'new luxury apartment private outside patio g...\n",
       "                               ...                        \n",
       "55279    b'large private double bedroom available brand...\n",
       "55280                                                  b''\n",
       "55281    b'double bedroom apartment family friend fully...\n",
       "55282    b'amazing bed apartment offer ideal base trave...\n",
       "55283    b'spacious bedroom beautiful ensuite perfect b...\n",
       "Name: summary, Length: 55270, dtype: object"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_features['summary'].str.encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10271"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20288"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features['summary']\n",
    "vocab_temp = []\n",
    "for c in features.summary.tolist():\n",
    "    vocab_temp.extend(c)\n",
    "# vocab_temp.extend([c for c in features['summary'].tolist()])\n",
    "len(set(vocab_temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = text_features['summary'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Vectorizer = tf.keras.layers.TextVectorization()\n",
    "\n",
    "\n",
    "#fit the vectorizer on the text and extract the corpus vocabulary\n",
    "Vectorizer.adapt(text_features['summary'])\n",
    "# vocab = weights\n",
    "\n",
    "\n",
    "#generate the embedding matrix\n",
    "num_tokens = len(vocab)\n",
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for i, word in enumerate(vocab):\n",
    "    embedding_matrix[i] = weights.get_vector(word)\n",
    "\n",
    "#Load the embedding matrix as the weights matrix for the embedding layer and set trainable to False\n",
    "Embedding_layer=layers.Embedding(\n",
    "    num_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_custom_text_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Input(shape=(1,), dtype=tf.string))\n",
    "    model.add(Vectorizer)\n",
    "    model.add(Embedding_layer)\n",
    "    model.add(layers.LSTM(16, return_sequences=False))\n",
    "    model.add(layers.Dense(1, activation=\"relu\"))\n",
    "    return model\n",
    "\n",
    "temp_model = create_custom_text_model()\n",
    "temp_model.predict([t]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_desc = train['description'].fillna('empty')\n",
    "train_ngbr = train['neighborhood_overview'].fillna('empty')\n",
    "\n",
    "y_train = train['price']\n",
    "\n",
    "\n",
    "print(len(train_desc), len(train_ngbr), len(y_train))\n",
    "\n",
    "\n",
    "def our_standardization(text_data):\n",
    "  lowercase = tf.strings.lower(text_data) # convert to lowercase\n",
    "  remove_html = tf.strings.regex_replace(lowercase, '<br />', ' ') # remove HTML tags\n",
    "  pattern_remove_punctuation = '[%s]' % re.escape(string.punctuation) # pattern to remove punctuation\n",
    "  remove_punct = tf.strings.regex_replace(remove_html, pattern_remove_punctuation, '') # apply pattern\n",
    "  remove_double_spaces = tf.strings.regex_replace(remove_punct, '\\s+', ' ') # remove double space\n",
    "  return remove_double_spaces\n",
    "\n",
    "\n",
    "\n",
    "vocab_size = 10000\n",
    "seq_length = 500\n",
    "\n",
    "# Create a vectorization layer\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize = our_standardization,\n",
    "    max_tokens = vocab_size,\n",
    "    output_sequence_length = seq_length\n",
    "    )\n",
    "\n",
    "emb_size = 32\n",
    "rnn_units = 16\n",
    "\n",
    "\n",
    "def create_text_model(text_list):\n",
    "    model = layers.Sequential()\n",
    "    model.add(layers.Input(shape=(1,), dtype=tf.string))\n",
    "    vectorize_layer.adapt(text_list)\n",
    "    model.add(vectorize_layer)\n",
    "    model.add(layers.Embedding(input_dim=vocab_size, output_dim=emb_size))\n",
    "    model.add(layers.LSTM(rnn_units, return_sequences=False))\n",
    "    model.add(layers.Dense(1, activation=\"relu\"))\n",
    "    return model\n",
    "\n",
    "def create_combined_model(X):\n",
    "    X = layers.Flatten()(X)\n",
    "    X = layers.Dense(1, activation=\"linear\")(X)\n",
    "    print(X.shape)\n",
    "    return X\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    \n",
    "    ### Initialize Input layers\n",
    "    input_desc = layers.Input(shape=(1,), dtype=tf.string)\n",
    "    input_ngbr = layers.Input(shape=(1,), dtype=tf.string)\n",
    "    \n",
    "    ### Create Vectorisation models from text features\n",
    "    desc_model = create_text_model(train_desc)\n",
    "    ngbr_model = create_text_model(train_ngbr)\n",
    "#     combined_model = create_combined_model()\n",
    "    \n",
    "    ### Create Data flow\n",
    "    emb_desc = desc_model(input_desc)\n",
    "    emb_ngbr = ngbr_model(input_ngbr)\n",
    "    concat_combined = layers.Concatenate()([emb_desc,emb_ngbr])\n",
    "    print(concat_combined.shape)\n",
    "    output = create_combined_model(concat_combined)\n",
    "    \n",
    "    ### Finalize the model\n",
    "    model = tf.keras.Model(inputs = [input_desc, input_ngbr], outputs = output)\n",
    "    model.compile(optimizer='adam',loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "\n",
    "model = create_model()\n",
    "print(model.summary())\n",
    "\n",
    "history = model.fit(\n",
    "    [train_desc, train_ngbr],\n",
    "    y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs = 5,\n",
    "    batch_size = 32,\n",
    "    verbose =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CNN-LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-21T12:08:25.228858Z",
     "iopub.status.busy": "2022-08-21T12:08:25.228441Z",
     "iopub.status.idle": "2022-08-21T12:08:25.251204Z",
     "shell.execute_reply": "2022-08-21T12:08:25.250280Z",
     "shell.execute_reply.started": "2022-08-21T12:08:25.228824Z"
    },
    "id": "AXLd6YufyO0E",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_desc = train['description'].fillna('empty')\n",
    "train_ngbr = train['neighborhood_overview'].fillna('empty')\n",
    "\n",
    "y_train = train['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-21T12:08:36.375059Z",
     "iopub.status.busy": "2022-08-21T12:08:36.374668Z",
     "iopub.status.idle": "2022-08-21T12:08:36.380855Z",
     "shell.execute_reply": "2022-08-21T12:08:36.379818Z",
     "shell.execute_reply.started": "2022-08-21T12:08:36.375027Z"
    },
    "id": "rYaQferdbXTJ",
    "outputId": "08fbe684-16c5-495c-a155-7607859ba140",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(len(train_desc), len(train_ngbr), len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DhhXJXfd0CfK",
    "outputId": "9540095b-d54b-4a34-c3d1-0d503ff0f7df"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-21T10:21:55.322484Z",
     "iopub.status.busy": "2022-08-21T10:21:55.322131Z",
     "iopub.status.idle": "2022-08-21T10:21:55.328779Z",
     "shell.execute_reply": "2022-08-21T10:21:55.327481Z",
     "shell.execute_reply.started": "2022-08-21T10:21:55.322455Z"
    },
    "id": "neO0GCjVXRKN",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def our_standardization(text_data):\n",
    "  lowercase = tf.strings.lower(text_data) # convert to lowercase\n",
    "  remove_html = tf.strings.regex_replace(lowercase, '<br />', ' ') # remove HTML tags\n",
    "  pattern_remove_punctuation = '[%s]' % re.escape(string.punctuation) # pattern to remove punctuation\n",
    "  remove_punct = tf.strings.regex_replace(remove_html, pattern_remove_punctuation, '') # apply pattern\n",
    "  remove_double_spaces = tf.strings.regex_replace(remove_punct, '\\s+', ' ') # remove double space\n",
    "  return remove_double_spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-21T10:22:00.749352Z",
     "iopub.status.busy": "2022-08-21T10:22:00.749007Z",
     "iopub.status.idle": "2022-08-21T10:22:03.278215Z",
     "shell.execute_reply": "2022-08-21T10:22:03.277245Z",
     "shell.execute_reply.started": "2022-08-21T10:22:00.749322Z"
    },
    "id": "vKXEnqhiX6x2",
    "outputId": "c5169b05-08f0-4479-8973-df1c787779ce",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "our_standardization(train_desc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-21T10:22:04.665006Z",
     "iopub.status.busy": "2022-08-21T10:22:04.664110Z",
     "iopub.status.idle": "2022-08-21T10:22:04.709687Z",
     "shell.execute_reply": "2022-08-21T10:22:04.708770Z",
     "shell.execute_reply.started": "2022-08-21T10:22:04.664960Z"
    },
    "id": "KXR3N4xgyR_9",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "vocab_size = 10000\n",
    "seq_length = 500\n",
    "\n",
    "# Create a vectorization layer\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize = our_standardization,\n",
    "    max_tokens = vocab_size,\n",
    "    output_sequence_length = seq_length\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-21T10:22:20.032495Z",
     "iopub.status.busy": "2022-08-21T10:22:20.029506Z",
     "iopub.status.idle": "2022-08-21T10:22:20.042152Z",
     "shell.execute_reply": "2022-08-21T10:22:20.041063Z",
     "shell.execute_reply.started": "2022-08-21T10:22:20.032449Z"
    },
    "id": "JbOBJHa2c1nk",
    "outputId": "172ffda2-f1da-477b-c783-6a17fae1df61",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "desc1, desc2 = train_desc[2], train_desc[3]\n",
    "print(desc1+'\\n\\n'+desc2)\n",
    "\n",
    "ngbr1, ngbr2 = train_ngbr[2], train_ngbr[3]\n",
    "print(ngbr1+'\\n\\n'+ngbr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-21T10:24:27.119548Z",
     "iopub.status.busy": "2022-08-21T10:24:27.119007Z",
     "iopub.status.idle": "2022-08-21T10:24:28.247474Z",
     "shell.execute_reply": "2022-08-21T10:24:28.244830Z",
     "shell.execute_reply.started": "2022-08-21T10:24:27.119509Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "\n",
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-21T11:49:38.541979Z",
     "iopub.status.busy": "2022-08-21T11:49:38.541012Z",
     "iopub.status.idle": "2022-08-21T11:49:38.591117Z",
     "shell.execute_reply": "2022-08-21T11:49:38.590057Z",
     "shell.execute_reply.started": "2022-08-21T11:49:38.541937Z"
    },
    "id": "ovNC3cDWfmK9",
    "outputId": "38ea07cd-25cc-4a2d-d5f1-3c2e33bbe085",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "emb_size = 32\n",
    "rnn_units = 16\n",
    "\n",
    "\n",
    "def create_text_model(text_list):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(1,), dtype=tf.string))\n",
    "    vectorize_layer.adapt(text_list)\n",
    "    model.add(vectorize_layer)\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=emb_size))\n",
    "    model.add(LSTM(rnn_units, return_sequences=False))\n",
    "    model.add(Dense(1, activation=\"relu\"))\n",
    "    return model\n",
    "\n",
    "def create_combined_model(X):\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(1, activation=\"linear\")(X)\n",
    "    print(X.shape)\n",
    "    return X\n",
    "\n",
    "def create_model():\n",
    "    \n",
    "    ### Initialize Input layers\n",
    "    input_desc = Input(shape=(1,), dtype=tf.string)\n",
    "    input_ngbr = Input(shape=(1,), dtype=tf.string)\n",
    "    \n",
    "    ### Create Vectorisation models from text features\n",
    "    desc_model = create_text_model(train_desc)\n",
    "    ngbr_model = create_text_model(train_ngbr)\n",
    "#     combined_model = create_combined_model()\n",
    "    \n",
    "    ### Create Data flow\n",
    "    emb_desc = desc_model(input_desc)\n",
    "    emb_ngbr = ngbr_model(input_ngbr)\n",
    "    concat_combined = Concatenate()([emb_desc,emb_ngbr])\n",
    "    print(concat_combined.shape)\n",
    "    output = create_combined_model(concat_combined)\n",
    "    \n",
    "    ### Finalize the model\n",
    "    model = Model(inputs = [input_desc, input_ngbr], outputs = output)\n",
    "    model.compile(optimizer='adam',loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# desc_embedded = desc_model.predict([desc1,desc2])\n",
    "# ngbr_model = create_text_model(train_ngbr)\n",
    "\n",
    "# dense_desc = desc_model([desc2])\n",
    "# dense_ngbr = ngbr_model([ngbr2])\n",
    "\n",
    "# concatenated_values = layers.Concatenate([dense_model, ngbr_model])\n",
    "\n",
    "# print(concatenated_values.shape)\n",
    "\n",
    "# def create_model(text_features_list):\n",
    "#   vec_model = \n",
    "#   for feature in text_features_list:\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VZJkZeLmyc4m",
    "outputId": "bc1eb455-92a4-4e4d-97a4-5e7ee2fc4d28"
   },
   "outputs": [],
   "source": [
    "# Create model with LSTM\n",
    "emb_size = 100\n",
    "rnn_units = 64\n",
    "\n",
    "input_ngbr = tf.keras.Input(shape=(seq_length,), dtype=\"int64\") \n",
    "input_desc = tf.keras.Input(shape=(seq_length,), dtype=\"int64\") \n",
    "emb_desc = layers.Embedding(input_dim=vocab_size, output_dim=emb_size)(input_desc) \n",
    "x_desc = layers.GRU(rnn_units)(emb_desc)\n",
    "dense_desc = layers.Dense(1, activation=\"relu\")(x_desc)\n",
    "\n",
    "emb_ngbr = layers.Embedding(input_dim=vocab_size, output_dim=emb_size)(input_ngbr) \n",
    "x_ngbr = layers.GRU(rnn_units)(emb_ngbr)\n",
    "dense_ngbr = layers.Dense(1, activation=\"relu\")(x_ngbr)\n",
    "\n",
    "concat = concatenate([dense_desc, dense_ngbr])\n",
    "norm = layers.BatchNormalization()(concat)\n",
    "dense_full = Dense(128, activation=\"relu\")(norm)\n",
    "#dense_full = Dense(64, activation=\"relu\")(dense_full)\n",
    "output_layer = Dense(1, activation=\"relu\")(dense_full)\n",
    "\n",
    "\n",
    "model = tf.keras.Model(inputs = [input_desc, input_ngbr], outputs = output_layer) \n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "    loss='mse', \n",
    "    metrics=['mae']) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-21T11:45:44.268574Z",
     "iopub.status.busy": "2022-08-21T11:45:44.267969Z",
     "iopub.status.idle": "2022-08-21T11:45:44.292924Z",
     "shell.execute_reply": "2022-08-21T11:45:44.291766Z",
     "shell.execute_reply.started": "2022-08-21T11:45:44.268540Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# tf.convert_to_tensor([zip(train_desc,train_ngbr))\n",
    "y_train = tf.convert_to_tensor(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-21T12:00:50.616792Z",
     "iopub.status.busy": "2022-08-21T12:00:50.616158Z",
     "iopub.status.idle": "2022-08-21T12:00:50.634243Z",
     "shell.execute_reply": "2022-08-21T12:00:50.633182Z",
     "shell.execute_reply.started": "2022-08-21T12:00:50.616734Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_text_ds_raw = tf.data.Dataset.from_tensor_slices(\n",
    "            tf.cast(train_features.values, tf.string)\n",
    ") \n",
    "train_cat_ds_raw = tf.data.Dataset.from_tensor_slices(\n",
    "            tf.cast(train_targets.values, tf.int64),\n",
    "\n",
    ") \n",
    "\n",
    "\n",
    "def convert_text_input(sample):\n",
    "    text = sample\n",
    "    text = tf.expand_dims(text, -1)  \n",
    "    #return tf.squeeze(vectorize_layer(text))\n",
    "    return tf.squeeze(vectorize_layer(text))\n",
    "\n",
    "convert_text_input([\"what is this misery\",\"what is your story\"]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-21T12:12:17.219588Z",
     "iopub.status.busy": "2022-08-21T12:12:17.219203Z",
     "iopub.status.idle": "2022-08-21T12:18:41.882868Z",
     "shell.execute_reply": "2022-08-21T12:18:41.881803Z",
     "shell.execute_reply.started": "2022-08-21T12:12:17.219556Z"
    },
    "id": "u1Mo2U2P5BcS",
    "outputId": "d1de1e16-2c54-4ea8-ef25-f10996a6e02c",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# model = create_model()\n",
    "# desc_model = create_text_model(train_desc)\n",
    "\n",
    "# print(desc_model.summary())\n",
    "# desc_model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "story = model.fit(\n",
    "   [train_desc, train_ngbr],\n",
    "   y_train,\n",
    "#     validation_split=0.2,\n",
    "    epochs = 5,\n",
    "    batch_size = 32,\n",
    "    verbose =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "52634da84371cba311ea128a5ea7cdc41ff074b781779e754b270ff9f8153cee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
